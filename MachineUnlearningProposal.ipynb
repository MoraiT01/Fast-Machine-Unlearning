{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9584dc74",
   "metadata": {},
   "source": [
    "# Machine Unlearning + Noise Generator\n",
    "\n",
    "This is a copy of the original `Machine Unlearning.ipynb` notebook, with the key difference of using a different way of generating the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e828435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "train_new_one = False\n",
    "# torch.manual_seed(100)\n",
    "# After I optimize the Hyperparameters, I want to calculate at least 30 models, to chech the average performance\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73d496",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e04a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def training_step(model, batch):\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "    out = model(images)                  \n",
    "    loss = F.cross_entropy(out, labels) \n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch):\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "    out = model(images)                    \n",
    "    loss = F.cross_entropy(out, labels)   \n",
    "    acc = accuracy(out, labels)\n",
    "    return {'Loss': loss.detach(), 'Acc': acc}\n",
    "\n",
    "def validation_epoch_end(model, outputs):\n",
    "    batch_losses = [x['Loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   \n",
    "    batch_accs = [x['Acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()      \n",
    "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(model, epoch, result):\n",
    "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "        epoch, result['lrs'][-1], result['train_loss'], result['Loss'], result['Acc']))\n",
    "    \n",
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec89a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
    "    return validation_epoch_end(model, outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = training_step(model, batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            \n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        sched.step(result['Loss'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6d890",
   "metadata": {},
   "source": [
    "## Train/Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32155eed",
   "metadata": {},
   "source": [
    "### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41e0a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./cifar10.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14598/3544667939.py:8: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path='./data')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n",
      "['bird', 'deer', 'horse', 'automobile', 'frog', 'airplane', 'truck', 'cat', 'dog', 'ship']\n"
     ]
    }
   ],
   "source": [
    "# Dowload the dataset\n",
    "if os.path.exists(\"data/cifar10\"):\n",
    "    dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "    download_url(dataset_url, '.')\n",
    "\n",
    "    # Extract from archive\n",
    "    with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "        tar.extractall(path='./data')\n",
    "        \n",
    "    # Look into the data directory\n",
    "    data_dir = './data/cifar10'\n",
    "    print(os.listdir(data_dir))\n",
    "    classes = os.listdir(data_dir + \"/train\")\n",
    "    print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29db69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a417a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(data_dir+'/train', transform_train)\n",
    "valid_ds = ImageFolder(data_dir+'/test', transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7844cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f4d2b",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54996a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = resnet18(num_classes = 10).to(DEVICE)\n",
    "\n",
    "epochs = 40\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6352284",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.exists(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"):\n",
    "    history = fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                                grad_clip=grad_clip, \n",
    "                                weight_decay=weight_decay, \n",
    "                                opt_func=opt_func)\n",
    "\n",
    "    torch.save(model.state_dict(), \"ResNET18_CIFAR10_ALL_CLASSES.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980397d",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3769eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new_one:\n",
    "    model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\", map_location=DEVICE))\n",
    "    history = [evaluate(model, valid_dl)]\n",
    "    history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e31ccb",
   "metadata": {},
   "source": [
    "## Unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4696560",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d212fd",
   "metadata": {},
   "source": [
    "Originally used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f88a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # defining the noise structure\n",
    "# class Noise(nn.Module):\n",
    "#     def __init__(self, *dim):\n",
    "#         super().__init__()\n",
    "#         self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
    "        \n",
    "#     def forward(self):\n",
    "#         return self.noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c939dd9",
   "metadata": {},
   "source": [
    "Trying a different approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287dac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network module for generating noise patterns\n",
    "    through a series of fully connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim_out: list,\n",
    "            dim_hidden: list = [1000],\n",
    "            dim_start: int = 100,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize the NoiseGenerator.\n",
    "\n",
    "        Parameters:\n",
    "            dim_out (list): The output dimensions for the generated noise.\n",
    "            dim_hidden (list): The dimensions of hidden layers, defaults to [1000].\n",
    "            dim_start (int): The initial dimension of random noise, defaults to 100.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim_out\n",
    "        self.start_dims = dim_start  # Initial dimension of random noise\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.layers = {}\n",
    "        self.layers[\"l1\"] = nn.Linear(self.start_dims, dim_hidden[0])\n",
    "        last = dim_hidden[0]\n",
    "        for idx in range(len(dim_hidden)-1):\n",
    "            self.layers[f\"l{idx+2}\"] = nn.Linear(dim_hidden[idx], dim_hidden[idx+1])\n",
    "            last = dim_hidden[idx+1]\n",
    "\n",
    "        # Define output layer\n",
    "        self.f_out = nn.Linear(last, math.prod(self.dim))        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward pass to transform random noise into structured output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The reshaped tensor with specified output dimensions.\n",
    "        \"\"\"\n",
    "        # Generate random starting noise\n",
    "        x = torch.randn(self.start_dims)\n",
    "        x = x.flatten()\n",
    "\n",
    "        # Transform noise into learnable patterns\n",
    "        for layer in self.layers.keys():\n",
    "            x = self.layers[layer](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # Apply output layer\n",
    "        x = self.f_out(x)\n",
    "\n",
    "        # Reshape tensor to the specified dimensions\n",
    "        reshaped_tensor = x.view(self.dim)\n",
    "        return reshaped_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff85afe",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65082f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all classes\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# classes which are required to un-learn\n",
    "classes_to_forget = [0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfedd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classwise list of samples\n",
    "num_classes = 10\n",
    "classwise_train = {}\n",
    "for i in range(num_classes):\n",
    "    classwise_train[i] = []\n",
    "\n",
    "for img, label in train_ds:\n",
    "    classwise_train[label].append((img, label))\n",
    "    \n",
    "classwise_test = {}\n",
    "for i in range(num_classes):\n",
    "    classwise_test[i] = []\n",
    "\n",
    "for img, label in valid_ds:\n",
    "    classwise_test[label].append((img, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edbda37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting some samples from retain classes\n",
    "num_samples_per_class = 1000\n",
    "\n",
    "retain_samples = []\n",
    "for i in range(len(classes)):\n",
    "    if classes[i] not in classes_to_forget:\n",
    "        retain_samples += classwise_train[i][:num_samples_per_class]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70736605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain validation set\n",
    "retain_valid = []\n",
    "for cls in range(num_classes):\n",
    "    if cls not in classes_to_forget:\n",
    "        for img, label in classwise_test[cls]:\n",
    "            retain_valid.append((img, label))\n",
    "            \n",
    "# forget validation set\n",
    "forget_valid = []\n",
    "for cls in range(num_classes):\n",
    "    if cls in classes_to_forget:\n",
    "        for img, label in classwise_test[cls]:\n",
    "            forget_valid.append((img, label))\n",
    "            \n",
    "forget_valid_dl = DataLoader(forget_valid, batch_size, num_workers=3, pin_memory=True)\n",
    "retain_valid_dl = DataLoader(retain_valid, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9afbe",
   "metadata": {},
   "source": [
    "### Training the Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fcc11a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14598/1124881384.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\", map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model\n",
    "model = resnet18(num_classes = 10).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\", map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1170217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 0 ns, total: 2 μs\n",
      "Wall time: 5.25 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if train_new_one:\n",
    "    noises = {}\n",
    "    for cls in classes_to_forget:\n",
    "        print(\"Optiming loss for class {}\".format(cls))\n",
    "        noises[cls] = Noise(batch_size, 3, 32, 32)\n",
    "        opt = torch.optim.Adam(noises[cls].parameters(), lr = 0.1)\n",
    "\n",
    "        num_epochs = 5\n",
    "        num_steps = 8\n",
    "        class_label = cls\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = []\n",
    "            for batch in range(num_steps):\n",
    "                inputs = noises[cls]()\n",
    "                labels = torch.zeros(batch_size)+class_label\n",
    "                outputs = model(inputs)\n",
    "                loss = -F.cross_entropy(outputs, labels.long()) + 0.1*torch.mean(torch.sum(torch.square(inputs), [1, 2, 3]))\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_loss.append(loss.cpu().detach().numpy())\n",
    "            print(\"Loss: {}\".format(np.mean(total_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08aa35",
   "metadata": {},
   "source": [
    "## Impair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09feaed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noises' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:8\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noises' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 256\n",
    "noisy_data = []\n",
    "num_batches = 20\n",
    "class_num = 0\n",
    "\n",
    "for cls in classes_to_forget:\n",
    "    for i in range(num_batches):\n",
    "        batch = noises[cls]().cpu().detach()\n",
    "        for i in range(batch[0].size(0)):\n",
    "            noisy_data.append((batch[i], torch.tensor(class_num)))\n",
    "\n",
    "other_samples = []\n",
    "for i in range(len(retain_samples)):\n",
    "    other_samples.append((retain_samples[i][0].cpu(), torch.tensor(retain_samples[i][1])))\n",
    "noisy_data += other_samples\n",
    "noisy_loader = torch.utils.data.DataLoader(noisy_data, batch_size=256, shuffle = True)\n",
    "\n",
    "\n",
    "if train_new_one:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.02)\n",
    "\n",
    "    for epoch in range(1):  \n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0\n",
    "        for i, data in enumerate(noisy_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs,torch.tensor(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            out = torch.argmax(outputs.detach(),dim=1)\n",
    "            assert out.shape==labels.shape\n",
    "            running_acc += (labels==out).sum().item()\n",
    "        print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4a772",
   "metadata": {},
   "source": [
    "### Performance after Impair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfcffec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new_one:\n",
    "    print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "    history = [evaluate(model, forget_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
    "\n",
    "    print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "    history = [evaluate(model, retain_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdfc92",
   "metadata": {},
   "source": [
    "## Repair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca2abac7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'other_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'other_samples' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "heal_loader = torch.utils.data.DataLoader(other_samples, batch_size=256, shuffle = True)\n",
    "if train_new_one:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "    for epoch in range(1):  \n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0\n",
    "        for i, data in enumerate(heal_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs,torch.tensor(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            out = torch.argmax(outputs.detach(),dim=1)\n",
    "            assert out.shape==labels.shape\n",
    "            running_acc += (labels==out).sum().item()\n",
    "        print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee6e55",
   "metadata": {},
   "source": [
    "### Performance after Repair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e74aa345",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new_one:\n",
    "    print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "    history = [evaluate(model, forget_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
    "\n",
    "    print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "    history = [evaluate(model, retain_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c19a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def load_models_dict(path: str=\"data/new/models\") -> Dict[str, torch.nn.Module]:\n",
    "    de = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(num_classes = 10).to(de)\n",
    "    \n",
    "    # load all the models\n",
    "    md = {}\n",
    "    for list in os.listdir(path):\n",
    "        \n",
    "        model.load_state_dict(torch.load(f=os.path.join(path, list), map_location=DEVICE, weights_only=True))\n",
    "        model.eval()\n",
    "        md[len(md)] = model\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2281224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Loss': 1.3517661094665527, 'Acc': 0.7792420387268066}]\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 9.188455581665039\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 61.83837652206421\n",
      "Loss: 1.075740098953247\n",
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 9.611581802368164\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 69.4384753704071\n",
      "Loss: 0.8779372572898865\n"
     ]
    }
   ],
   "source": [
    "from src.fyemu_tunable import main\n",
    "\n",
    "if True:\n",
    "    model = main(\n",
    "        new_baseline=False\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(\"data/new/models\"):\n",
    "        os.makedirs(\"data/new/models\")\n",
    "    n = len(os.listdir(\"data/new/models\"))\n",
    "    torch.save(model.state_dict(), f\"data/new/models/ResNET18_CIFAR10_UN_{n}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11c592",
   "metadata": {},
   "source": [
    "___\n",
    "## Evaluate multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bee59dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tarfile\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Firstly, all the data\n",
    "class SubData(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.data[idx]\n",
    "\n",
    "        img = Image.open(f\"{path}\").convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        label = torch.tensor(label)\n",
    "        return img.to(self.device), label.to(self.device)\n",
    "\n",
    "data_dir = f'data{os.sep}cifar10'\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# classes which are required to un-learn\n",
    "classes_to_forget = [0, 2]\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_train = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb5e2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_all_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "valid_all_dl = DataLoader(valid_all_ds, 256,)\n",
    "\n",
    "train_all_ds = ImageFolder(data_dir+f'{os.sep}train', transform_train)\n",
    "train_all_dl = DataLoader(train_all_ds, 256,)\n",
    "\n",
    "rt_tr = {}\n",
    "for t, l in train_all_ds.imgs:\n",
    "    if l not in classes_to_forget:\n",
    "        rt_tr[len(rt_tr)] = (t, l)\n",
    "rt_vl = {}\n",
    "for t, l in valid_all_ds.imgs:\n",
    "    if l not in classes_to_forget:\n",
    "        rt_vl[len(rt_vl)] = (t, l)\n",
    "\n",
    "train_retain_ds = SubData(rt_tr, transform_train)\n",
    "valid_retain_ds = SubData(rt_vl, transform_test)\n",
    "\n",
    "train_retain_dl = DataLoader(train_retain_ds, 256, shuffle=True)\n",
    "valid_retain_dl = DataLoader(valid_retain_ds, 256*2)\n",
    "\n",
    "rt_tr = {}\n",
    "for t, l in train_all_ds.imgs:\n",
    "    if l in classes_to_forget:\n",
    "        rt_tr[len(rt_tr)] = (t, l)\n",
    "rt_vl = {}\n",
    "for t, l in valid_all_ds.imgs:\n",
    "    if l in classes_to_forget:\n",
    "        rt_vl[len(rt_vl)] = (t, l)\n",
    "\n",
    "train_forget_ds = SubData(rt_tr, transform_train)\n",
    "valid_forget_ds = SubData(rt_vl, transform_test)\n",
    "\n",
    "train_forget_dl = DataLoader(train_forget_ds, 256, shuffle=True)\n",
    "valid_forget_dl = DataLoader(train_forget_ds, 256*2)\n",
    "\n",
    "loaders = {\n",
    "    0: train_all_dl,\n",
    "    1: valid_all_dl,\n",
    "    2: train_retain_dl,\n",
    "    3: valid_retain_dl,\n",
    "    4: train_forget_dl,\n",
    "    5: valid_forget_dl,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5c37661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.metrics\n",
    "from src.fyemu_tunable import evaluate\n",
    "paper_ms    = load_models_dict(path=\"data/paper/models\")\n",
    "gemu_ms     = load_models_dict(path=\"data/new/models\")\n",
    "\n",
    "exact_ms    = load_models_dict(path=\"data/retrain/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdd61db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Loss': 2.125023365020752, 'Acc': 0.8007015585899353},\n",
       " 1: {'Loss': 3.0320842266082764, 'Acc': 0.643359363079071},\n",
       " 2: {'Loss': 0.00032785828807391226, 'Acc': 1.0},\n",
       " 3: {'Loss': 1.1749095916748047, 'Acc': 0.7978271245956421},\n",
       " 4: {'Loss': 10.64272403717041, 'Acc': 0.0},\n",
       " 5: {'Loss': 10.659263610839844, 'Acc': 0.0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run(model, loaders):\n",
    "    results = {}\n",
    "    for name, loader in loaders.items():\n",
    "        results[name] = evaluate(model.to(DEVICE), loader)\n",
    "\n",
    "    return results\n",
    "run(exact_ms[0], loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5922b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "kl_exact_to_fyemu = {key: src.metrics.kl_divergence_between_models(exact_ms[0], paper_ms[0], loader) for key, loader in loaders.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea4b7584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3.001258940112834,\n",
       " 1: 2.3969006985425954,\n",
       " 2: 2.9558742061542094,\n",
       " 3: 2.253737926483154,\n",
       " 4: 3.1943638205528258,\n",
       " 5: 3.2036361098289494}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_exact_to_fyemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ec23f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "kl_exact_to_gemu= {key: src.metrics.kl_divergence_between_models(exact_ms[0], gemu_ms[0], loader) for key, loader in loaders.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c45e669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3.1400943976275797,\n",
       " 1: 2.5975292026996617,\n",
       " 2: 3.17433748579329,\n",
       " 3: 2.5042692273855214,\n",
       " 4: 3.0013855218887335,\n",
       " 5: 3.023870861530304}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_exact_to_gemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "095208fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Literal\n",
    "\n",
    "def create_boxplots(score_lists1: Dict[str, List[float]], score_lists2: Dict[str, List[float]], score_lists3: Dict[str, List[float]], title: str = 'Box Plot of Accuracy Scores for Different Models', evaluation: Literal[\"Accuracy\", \"Loss\"] = \"Accuracy\") -> None:\n",
    "    \"\"\"Create a box plot of accuracy scores for each parsed list in the diconary.\"\"\"\n",
    "\n",
    "    # Prepare data for the box plot\n",
    "    data1 = [sum(scores)/len(scores) for scores in score_lists1.values()]\n",
    "    data2 = [sum(scores)/len(scores) for scores in score_lists2.values()]\n",
    "    data3 = [sum(scores)/len(scores) for scores in score_lists3.values()]\n",
    "    labels = list(score_lists1.keys())\n",
    "    X_axis = (np.arange(len(labels)))\n",
    "\n",
    "    # Create the box plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x=X_axis-0.3   , height=data1, width=0.3, color=\"blue\", label=\"Baseline\")\n",
    "    plt.bar(x=X_axis        , height=data2, width=0.3, color=\"red\", label=\"FYEMU\")\n",
    "    plt.bar(x=X_axis+0.3   , height=data3, width=0.3, color=\"orange\", label=\"GEMU\")\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xticks(X_axis, labels)\n",
    "    plt.xlabel('Subsets')\n",
    "    plt.ylabel(f'{evaluation} Score')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7b932",
   "metadata": {},
   "source": [
    "### Accuracy Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47c7551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tarfile\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_classes = {}\n",
    "for c in classes:\n",
    "    data_classes[c] = {}\n",
    "    for t, l in valid_all_ds.imgs:\n",
    "        if l == c:\n",
    "            data_classes[c][len(data_classes[c])] = (t, l)\n",
    "\n",
    "data_dl_classes = {c: DataLoader(SubData(data_classes[c], transform_test), 256) for c in classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4887f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accs_paper = {c: [] for c in classes}\n",
    "for idx, model in paper_ms.items():\n",
    "    model.eval()\n",
    "    accs = run(model, data_dl_classes)\n",
    "    for c, acc in accs.items():\n",
    "        class_accs_paper[c].append(acc[\"Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b73c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accs_gemu = {c: [] for c in classes}\n",
    "for idx, model in gemu_ms.items():\n",
    "    model.eval()\n",
    "    accs = run(model, data_dl_classes)\n",
    "    for c, acc in accs.items():\n",
    "        class_accs_gemu[c].append(acc[\"Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b134631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accs_exact = {c: [] for c in classes}\n",
    "for idx, model in exact_ms.items():\n",
    "    model.eval()\n",
    "    accs = run(model, data_dl_classes)\n",
    "    for c, acc in accs.items():\n",
    "        class_accs_exact[c].append(acc[\"Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83c8a7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_boxplots(class_accs_exact, class_accs_paper, class_accs_gemu, \"Box Plot of Accuracy Scores for FYEMU\", \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "974870e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7518858313560486)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delta Acc\n",
    "\n",
    "sum([np.absolute(sum(i)/len(i)-(sum(j)/len(j))) for i, j in zip(class_accs_exact.values(), class_accs_paper.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea719ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.151737630367279)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sum([np.absolute(sum(i)/len(i)-(sum(j)/len(j))) for i, j in zip(class_accs_exact.values(), class_accs_gemu.values())])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
