{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9584dc74",
   "metadata": {},
   "source": [
    "# Machine Unlearning + Noise Generator\n",
    "\n",
    "This is a copy of the original `Machine Unlearning.ipynb` notebook, with the key difference of using a different way of generating the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e828435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "train_new_one = False\n",
    "# torch.manual_seed(100)\n",
    "# After I optimize the Hyperparameters, I want to calculate at least 30 models, to chech the average performance\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73d496",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e04a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def training_step(model, batch):\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "    out = model(images)                  \n",
    "    loss = F.cross_entropy(out, labels) \n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch):\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "    out = model(images)                    \n",
    "    loss = F.cross_entropy(out, labels)   \n",
    "    acc = accuracy(out, labels)\n",
    "    return {'Loss': loss.detach(), 'Acc': acc}\n",
    "\n",
    "def validation_epoch_end(model, outputs):\n",
    "    batch_losses = [x['Loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   \n",
    "    batch_accs = [x['Acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()      \n",
    "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(model, epoch, result):\n",
    "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "        epoch, result['lrs'][-1], result['train_loss'], result['Loss'], result['Acc']))\n",
    "    \n",
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec89a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
    "    return validation_epoch_end(model, outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = training_step(model, batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            \n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        sched.step(result['Loss'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6d890",
   "metadata": {},
   "source": [
    "## Train/Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32155eed",
   "metadata": {},
   "source": [
    "### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41e0a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz to .\\cifar10.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135107811/135107811 [01:51<00:00, 1213290.81it/s]\n",
      "C:\\Users\\Moritz\\AppData\\Local\\Temp\\ipykernel_6488\\4096972669.py:7: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path='./data')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# Dowload the dataset\n",
    "if os.path.exists(\"data/cifar10\"):\n",
    "    dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "    download_url(dataset_url, '.')\n",
    "\n",
    "    # Extract from archive\n",
    "    with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "        tar.extractall(path='./data')\n",
    "        \n",
    "    # Look into the data directory\n",
    "    data_dir = './data/cifar10'\n",
    "    print(os.listdir(data_dir))\n",
    "    classes = os.listdir(data_dir + \"/train\")\n",
    "    print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29db69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a417a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(data_dir+'/train', transform_train)\n",
    "valid_ds = ImageFolder(data_dir+'/test', transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7844cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f4d2b",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54996a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = resnet18(num_classes = 10).to(DEVICE = DEVICE)\n",
    "\n",
    "epochs = 40\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6352284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moritz\\miniconda3\\envs\\bach.conda\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.01000, train_loss: 1.7336, val_loss: 1.4651, val_acc: 0.4667\n",
      "Epoch [1], last_lr: 0.01000, train_loss: 1.3112, val_loss: 1.3865, val_acc: 0.4814\n",
      "Epoch [2], last_lr: 0.01000, train_loss: 1.0793, val_loss: 1.0454, val_acc: 0.6320\n",
      "Epoch [3], last_lr: 0.01000, train_loss: 0.9245, val_loss: 1.0101, val_acc: 0.6419\n",
      "Epoch [4], last_lr: 0.01000, train_loss: 0.8252, val_loss: 0.9361, val_acc: 0.6735\n",
      "Epoch [5], last_lr: 0.01000, train_loss: 0.7704, val_loss: 0.8150, val_acc: 0.7235\n",
      "Epoch [6], last_lr: 0.01000, train_loss: 0.7177, val_loss: 0.8574, val_acc: 0.6986\n",
      "Epoch [7], last_lr: 0.01000, train_loss: 0.6850, val_loss: 0.8337, val_acc: 0.7151\n",
      "Epoch [8], last_lr: 0.01000, train_loss: 0.6535, val_loss: 0.8128, val_acc: 0.7266\n",
      "Epoch [9], last_lr: 0.01000, train_loss: 0.6278, val_loss: 0.8057, val_acc: 0.7249\n",
      "Epoch [10], last_lr: 0.01000, train_loss: 0.6010, val_loss: 0.7617, val_acc: 0.7429\n",
      "Epoch [11], last_lr: 0.01000, train_loss: 0.5831, val_loss: 0.8717, val_acc: 0.7117\n",
      "Epoch [12], last_lr: 0.01000, train_loss: 0.5673, val_loss: 0.8128, val_acc: 0.7277\n",
      "Epoch [13], last_lr: 0.01000, train_loss: 0.5440, val_loss: 0.8869, val_acc: 0.7168\n",
      "Epoch [14], last_lr: 0.01000, train_loss: 0.5363, val_loss: 0.7660, val_acc: 0.7421\n",
      "Epoch [15], last_lr: 0.00500, train_loss: 0.3758, val_loss: 0.7246, val_acc: 0.7639\n",
      "Epoch [16], last_lr: 0.00500, train_loss: 0.3224, val_loss: 0.8279, val_acc: 0.7476\n",
      "Epoch [17], last_lr: 0.00500, train_loss: 0.3030, val_loss: 0.8361, val_acc: 0.7457\n",
      "Epoch [18], last_lr: 0.00500, train_loss: 0.2840, val_loss: 0.8356, val_acc: 0.7511\n",
      "Epoch [19], last_lr: 0.00500, train_loss: 0.2662, val_loss: 0.8744, val_acc: 0.7525\n",
      "Epoch [20], last_lr: 0.00250, train_loss: 0.1331, val_loss: 0.9255, val_acc: 0.7725\n",
      "Epoch [21], last_lr: 0.00250, train_loss: 0.0759, val_loss: 1.0473, val_acc: 0.7657\n",
      "Epoch [22], last_lr: 0.00250, train_loss: 0.0801, val_loss: 1.1963, val_acc: 0.7558\n",
      "Epoch [23], last_lr: 0.00250, train_loss: 0.0965, val_loss: 1.1251, val_acc: 0.7610\n",
      "Epoch [24], last_lr: 0.00125, train_loss: 0.0455, val_loss: 1.0950, val_acc: 0.7731\n",
      "Epoch [25], last_lr: 0.00125, train_loss: 0.0151, val_loss: 1.1790, val_acc: 0.7721\n",
      "Epoch [26], last_lr: 0.00125, train_loss: 0.0066, val_loss: 1.2039, val_acc: 0.7739\n",
      "Epoch [27], last_lr: 0.00125, train_loss: 0.0045, val_loss: 1.2353, val_acc: 0.7726\n",
      "Epoch [28], last_lr: 0.00063, train_loss: 0.0028, val_loss: 1.2543, val_acc: 0.7765\n",
      "Epoch [29], last_lr: 0.00063, train_loss: 0.0025, val_loss: 1.2772, val_acc: 0.7740\n",
      "Epoch [30], last_lr: 0.00063, train_loss: 0.0022, val_loss: 1.2867, val_acc: 0.7749\n",
      "Epoch [31], last_lr: 0.00063, train_loss: 0.0017, val_loss: 1.3013, val_acc: 0.7726\n",
      "Epoch [32], last_lr: 0.00031, train_loss: 0.0015, val_loss: 1.2996, val_acc: 0.7745\n",
      "Epoch [33], last_lr: 0.00031, train_loss: 0.0014, val_loss: 1.3112, val_acc: 0.7736\n",
      "Epoch [34], last_lr: 0.00031, train_loss: 0.0012, val_loss: 1.3180, val_acc: 0.7744\n",
      "Epoch [35], last_lr: 0.00031, train_loss: 0.0012, val_loss: 1.3347, val_acc: 0.7749\n",
      "Epoch [36], last_lr: 0.00016, train_loss: 0.0011, val_loss: 1.3278, val_acc: 0.7745\n",
      "Epoch [37], last_lr: 0.00016, train_loss: 0.0011, val_loss: 1.3296, val_acc: 0.7736\n",
      "Epoch [38], last_lr: 0.00016, train_loss: 0.0010, val_loss: 1.3308, val_acc: 0.7741\n",
      "Epoch [39], last_lr: 0.00016, train_loss: 0.0010, val_loss: 1.3404, val_acc: 0.7736\n",
      "CPU times: total: 18h 57min 25s\n",
      "Wall time: 3h 43min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.exists(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"):\n",
    "    history = fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                                grad_clip=grad_clip, \n",
    "                                weight_decay=weight_decay, \n",
    "                                opt_func=opt_func)\n",
    "\n",
    "    torch.save(model.state_dict(), \"ResNET18_CIFAR10_ALL_CLASSES.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980397d",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3769eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moritz\\AppData\\Local\\Temp\\ipykernel_6488\\2337143943.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Loss': 1.340433955192566, 'Acc': 0.7736040949821472}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if train_new_one:\n",
    "    model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"))\n",
    "    history = [evaluate(model, valid_dl)]\n",
    "    history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e31ccb",
   "metadata": {},
   "source": [
    "## Unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4696560",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d212fd",
   "metadata": {},
   "source": [
    "Originally used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f88a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # defining the noise structure\n",
    "# class Noise(nn.Module):\n",
    "#     def __init__(self, *dim):\n",
    "#         super().__init__()\n",
    "#         self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
    "        \n",
    "#     def forward(self):\n",
    "#         return self.noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c939dd9",
   "metadata": {},
   "source": [
    "Trying a different approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "287dac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network module for generating noise patterns\n",
    "    through a series of fully connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim_out: list,\n",
    "            dim_hidden: list = [1000],\n",
    "            dim_start: int = 100,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize the NoiseGenerator.\n",
    "\n",
    "        Parameters:\n",
    "            dim_out (list): The output dimensions for the generated noise.\n",
    "            dim_hidden (list): The dimensions of hidden layers, defaults to [1000].\n",
    "            dim_start (int): The initial dimension of random noise, defaults to 100.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim_out\n",
    "        self.start_dims = dim_start  # Initial dimension of random noise\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.layers = {}\n",
    "        self.layers[\"l1\"] = nn.Linear(self.start_dims, dim_hidden[0])\n",
    "        last = dim_hidden[0]\n",
    "        for idx in range(len(dim_hidden)-1):\n",
    "            self.layers[f\"l{idx+2}\"] = nn.Linear(dim_hidden[idx], dim_hidden[idx+1])\n",
    "            last = dim_hidden[idx+1]\n",
    "\n",
    "        # Define output layer\n",
    "        self.f_out = nn.Linear(last, math.prod(self.dim))        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward pass to transform random noise into structured output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The reshaped tensor with specified output dimensions.\n",
    "        \"\"\"\n",
    "        # Generate random starting noise\n",
    "        x = torch.randn(self.start_dims)\n",
    "        x = x.flatten()\n",
    "\n",
    "        # Transform noise into learnable patterns\n",
    "        for layer in self.layers.keys():\n",
    "            x = self.layers[layer](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # Apply output layer\n",
    "        x = self.f_out(x)\n",
    "\n",
    "        # Reshape tensor to the specified dimensions\n",
    "        reshaped_tensor = x.view(self.dim)\n",
    "        return reshaped_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff85afe",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65082f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all classes\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# classes which are required to un-learn\n",
    "classes_to_forget = [0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfedd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classwise list of samples\n",
    "num_classes = 10\n",
    "classwise_train = {}\n",
    "for i in range(num_classes):\n",
    "    classwise_train[i] = []\n",
    "\n",
    "for img, label in train_ds:\n",
    "    classwise_train[label].append((img, label))\n",
    "    \n",
    "classwise_test = {}\n",
    "for i in range(num_classes):\n",
    "    classwise_test[i] = []\n",
    "\n",
    "for img, label in valid_ds:\n",
    "    classwise_test[label].append((img, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edbda37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting some samples from retain classes\n",
    "num_samples_per_class = 1000\n",
    "\n",
    "retain_samples = []\n",
    "for i in range(len(classes)):\n",
    "    if classes[i] not in classes_to_forget:\n",
    "        retain_samples += classwise_train[i][:num_samples_per_class]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70736605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain validation set\n",
    "retain_valid = []\n",
    "for cls in range(num_classes):\n",
    "    if cls not in classes_to_forget:\n",
    "        for img, label in classwise_test[cls]:\n",
    "            retain_valid.append((img, label))\n",
    "            \n",
    "# forget validation set\n",
    "forget_valid = []\n",
    "for cls in range(num_classes):\n",
    "    if cls in classes_to_forget:\n",
    "        for img, label in classwise_test[cls]:\n",
    "            forget_valid.append((img, label))\n",
    "            \n",
    "forget_valid_dl = DataLoader(forget_valid, batch_size, num_workers=3, pin_memory=True)\n",
    "retain_valid_dl = DataLoader(retain_valid, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9afbe",
   "metadata": {},
   "source": [
    "### Training the Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fcc11a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moritz\\AppData\\Local\\Temp\\ipykernel_6488\\3732269000.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model\n",
    "model = resnet18(num_classes = 10).to(DEVICE = DEVICE)\n",
    "model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1170217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optiming loss for class 0\n",
      "Loss: 157.0191650390625\n",
      "Loss: -5.606943130493164\n",
      "Loss: -50.721160888671875\n",
      "Loss: -64.77427673339844\n",
      "Loss: -72.21408081054688\n",
      "Optiming loss for class 2\n",
      "Loss: 162.8887939453125\n",
      "Loss: 3.154118537902832\n",
      "Loss: -39.9158935546875\n",
      "Loss: -52.51765441894531\n",
      "Loss: -59.14886474609375\n",
      "CPU times: total: 12min 6s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if train_new_one:\n",
    "    noises = {}\n",
    "    for cls in classes_to_forget:\n",
    "        print(\"Optiming loss for class {}\".format(cls))\n",
    "        noises[cls] = Noise(batch_size, 3, 32, 32)\n",
    "        opt = torch.optim.Adam(noises[cls].parameters(), lr = 0.1)\n",
    "\n",
    "        num_epochs = 5\n",
    "        num_steps = 8\n",
    "        class_label = cls\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = []\n",
    "            for batch in range(num_steps):\n",
    "                inputs = noises[cls]()\n",
    "                labels = torch.zeros(batch_size)+class_label\n",
    "                outputs = model(inputs)\n",
    "                loss = -F.cross_entropy(outputs, labels.long()) + 0.1*torch.mean(torch.sum(torch.square(inputs), [1, 2, 3]))\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_loss.append(loss.cpu().detach().numpy())\n",
    "            print(\"Loss: {}\".format(np.mean(total_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08aa35",
   "metadata": {},
   "source": [
    "## Impair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09feaed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1: 0.15092642689704894,Train Acc:11.626%\n",
      "CPU times: total: 5min 2s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 256\n",
    "noisy_data = []\n",
    "num_batches = 20\n",
    "class_num = 0\n",
    "\n",
    "for cls in classes_to_forget:\n",
    "    for i in range(num_batches):\n",
    "        batch = noises[cls]().cpu().detach()\n",
    "        for i in range(batch[0].size(0)):\n",
    "            noisy_data.append((batch[i], torch.tensor(class_num)))\n",
    "\n",
    "other_samples = []\n",
    "for i in range(len(retain_samples)):\n",
    "    other_samples.append((retain_samples[i][0].cpu(), torch.tensor(retain_samples[i][1])))\n",
    "noisy_data += other_samples\n",
    "noisy_loader = torch.utils.data.DataLoader(noisy_data, batch_size=256, shuffle = True)\n",
    "\n",
    "\n",
    "if train_new_one:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.02)\n",
    "\n",
    "    for epoch in range(1):  \n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0\n",
    "        for i, data in enumerate(noisy_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs,torch.tensor(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            out = torch.argmax(outputs.detach(),dim=1)\n",
    "            assert out.shape==labels.shape\n",
    "            running_acc += (labels==out).sum().item()\n",
    "        print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4a772",
   "metadata": {},
   "source": [
    "### Performance after Impair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfcffec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 10.015697479248047\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 64.73388671875\n",
      "Loss: 0.9795756340026855\n"
     ]
    }
   ],
   "source": [
    "if train_new_one:\n",
    "    print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "    history = [evaluate(model, forget_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
    "\n",
    "    print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "    history = [evaluate(model, retain_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdfc92",
   "metadata": {},
   "source": [
    "## Repair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca2abac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1: 0.0940812198638916,Train Acc:12.544%\n",
      "CPU times: total: 4min 50s\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "heal_loader = torch.utils.data.DataLoader(other_samples, batch_size=256, shuffle = True)\n",
    "if train_new_one:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "    for epoch in range(1):  \n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0\n",
    "        for i, data in enumerate(heal_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs,torch.tensor(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            out = torch.argmax(outputs.detach(),dim=1)\n",
    "            assert out.shape==labels.shape\n",
    "            running_acc += (labels==out).sum().item()\n",
    "        print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee6e55",
   "metadata": {},
   "source": [
    "### Performance after Repair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e74aa345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Standard Forget Model on Forget Class\n",
      "Accuracy: 0.0\n",
      "Loss: 13.829742431640625\n",
      "Performance of Standard Forget Model on Retain Class\n",
      "Accuracy: 72.4047839641571\n",
      "Loss: 0.8255321979522705\n"
     ]
    }
   ],
   "source": [
    "if train_new_one:\n",
    "    print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "    history = [evaluate(model, forget_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
    "\n",
    "    print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "    history = [evaluate(model, retain_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def load_models_dict(path: str=\"data/new/models\") -> Dict[str, torch.nn.Module]:\n",
    "    \n",
    "    model = resnet18(num_classes = 10).to(DEVICE = DEVICE)\n",
    "    \n",
    "    # load all the models\n",
    "    md = {}\n",
    "    for list in os.listdir(path):\n",
    "        \n",
    "        model.load_state_dict(torch.load(f=os.path.join(path, list), weights_only=True))\n",
    "        model.eval()\n",
    "        md[len(md)] = model\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2281224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fyemu_tunable import main\n",
    "\n",
    "for i in range(10):\n",
    "    model = main()\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists(\"data/new/models\"):\n",
    "        os.makedirs(\"data/new/models\")\n",
    "    n = len(os.listdir(\"data/new/models\"))\n",
    "    torch.save(model.state_dict(), f\"data/new/models/ResNET18_CIFAR10_UN_{n}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11c592",
   "metadata": {},
   "source": [
    "___\n",
    "## Evaluate multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c37661",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ms = load_models_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5922b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divs_gen = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec23f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_femu = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095208fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(kl_divs_gen, kl_div_femu)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
