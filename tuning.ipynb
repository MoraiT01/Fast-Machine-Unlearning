{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Finetuning\n",
    "\n",
    "We want to find the right parameters for the Generator Network.\n",
    "\n",
    "By using the best values for the following parameters:\n",
    "\n",
    "- Number of Epochs (meaning `num_epochs` and `num_steps`)\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Number of Noise Batches\n",
    "- Number of Layers\n",
    "- Regularization term\n",
    "- Number of Neurons for each Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fyemu_tunable import main, evaluate\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import optuna\n",
    "import random\n",
    "from typing import Dict\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from src.metrics import kl_divergence_between_models\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_dict(path: str=\"data/new/models\") -> Dict[str, torch.nn.Module]:\n",
    "    de = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(num_classes = 10).to(de)\n",
    "    \n",
    "    # load all the models\n",
    "    md = {}\n",
    "    for list in os.listdir(path):\n",
    "        \n",
    "        model.load_state_dict(torch.load(f=os.path.join(path, list), weights_only=True))\n",
    "        model.eval()\n",
    "        md[len(md)] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this little tutorial, to see how we handle the optimization using save states:\n",
    "\n",
    "https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/001_rdb.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name = \"GeneratorOpti2\"  # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "\n",
    "if os.path.exists(\"sampler2.pkl\"):\n",
    "    restored_sampler = pickle.load(open(\"sampler2.pkl\", \"rb\"))\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, sampler=restored_sampler)\n",
    "else:\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Steps = trial.suggest_int('opt_Steps', 1, 20)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 512)\n",
    "    opt_Number_of_Noise_Batches = trial.suggest_int('opt_Number_of_Noise_Batches', 1, 10)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "\n",
    "    # print(f\"Epochs: {opt_Epochs} |\\nSteps: {opt_Steps} |\\nLearning Rate: {opt_Learning_Rate} |\\nBatch Size: {opt_Batch_Size} |\\nNoise Batches: {opt_Number_of_Noise_Batches} |\\nRegularization Term: {opt_Regularization_term} |\\nNoise Dim: {opt_Noise_Dim}\")\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 8)\n",
    "\n",
    "    Layers = [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,]\n",
    "    Layers = Layers[:n_layers]\n",
    "    # print(\"Layers: \", Layers)\n",
    "\n",
    "    mod = main(\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Steps= opt_Steps,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_Number_of_Noise_Batches = opt_Number_of_Noise_Batches,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        new_baseline=False,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "    \n",
    "    data_dir = f'data{os.sep}cifar10'\n",
    "\n",
    "    transform_test = tt.Compose([\n",
    "        tt.ToTensor(),\n",
    "        tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    valid_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "    valid_dl = DataLoader(valid_ds, 256,)\n",
    "\n",
    "    exact = resnet18(num_classes = 10)\n",
    "    n = random.randint(0, len(os.listdir(\"data/retrain/models\"))-1)\n",
    "    exact.load_state_dict(torch.load(f\"data/retrain/models/ResNET18_CIFAR10_RETRAIN_CLASSES_{n}.pt\", weights_only=True))\n",
    "    div = kl_divergence_between_models(\n",
    "        model1 = exact,\n",
    "        model2 = mod,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler2.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study.sampler, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df = study.trials_dataframe()\n",
    "best10_df = trials_df.sort_values(\"value\").head(10)\n",
    "best10_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Value {float(best10_df[\"value\"].mean())}\")\n",
    "print(f\"Number of Layers {float(best10_df[\"params_n_layers\"].mean())} \")\n",
    "print(f\"Batch Size for Training {float(best10_df[\"params_opt_Batch_Size\"].mean())} \")\n",
    "print(f\"Epochs for Noise Training {float(best10_df[\"params_opt_Epochs\"].mean())} \")\n",
    "print(f\"LR for Noise Training {float(best10_df[\"params_opt_Learning_Rate\"].mean())} \")\n",
    "print(f\"Noise Dim of Generator {float(best10_df[\"params_opt_Noise_Dim\"].mean())}\")\n",
    "print(f\"Number of Noise Batches Used {float(best10_df[\"params_opt_Number_of_Noise_Batches\"].mean())} \")\n",
    "print(f\"Regularization Term {float(best10_df[\"params_opt_Regularization_term\"].mean())} \")\n",
    "print(f\"Learning Steps for Noise Training {float(best10_df[\"params_opt_Steps\"].mean())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^This will represent the values used as default^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Standard Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n0 = 5000\n",
    "# n2 = 5000\n",
    "# batch_size = 128\n",
    "\n",
    "# standard_model, standard_history = main(\n",
    "#     t_Epochs = 5,\n",
    "#     t_Steps= int((n0 + n2)/(2 * batch_size)), # The Idea is to have the same amount of updates as their are samples to unlearn\n",
    "#     t_Learning_Rate = 0.1,\n",
    "#     t_Batch_Size = batch_size,\n",
    "#     t_Number_of_Noise_Batches = 10,\n",
    "#     t_Regularization_term = 0.1,\n",
    "#     t_Layers = [1000],\n",
    "#     t_Noise_Dim = 100,\n",
    "#     new_baseline=True,\n",
    "#     logs=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_ms = load_models_dict(path=\"data/all/models\")\n",
    "\n",
    "exact_ms = load_models_dict(path=\"data/retrain/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import kl_divergence_between_models\n",
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "batch_size = 256\n",
    "data_dir = f'data{os.sep}cifar10'\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "valid_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=False)\n",
    "\n",
    "kl_divergence_between_models(model1 = train_ms[0], model2 = train_ms[0], data_loader = valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=exact_ms[0], model2=exact_ms[0], data_loader=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=train_ms[0], model2=exact_ms[0], data_loader=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=exact_ms[0], model2=train_ms[0], data_loader=valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
