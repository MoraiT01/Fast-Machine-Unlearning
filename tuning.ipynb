{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Finetuning\n",
    "\n",
    "We want to find the right parameters for the Generator Network.\n",
    "\n",
    "By using the best values for the following parameters:\n",
    "\n",
    "- Number of Epochs (meaning `num_epochs` and `num_steps`)\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Number of Noise Batches\n",
    "- Number of Layers\n",
    "- Regularization term\n",
    "- Number of Neurons for each Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.fyemu_tunable import main, evaluate\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import optuna\n",
    "import random\n",
    "from typing import Dict\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from src.metrics import kl_divergence_between_models\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_dict(path: str=\"data/new/models\") -> Dict[str, torch.nn.Module]:\n",
    "    de = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(num_classes = 10).to(de)\n",
    "    \n",
    "    # load all the models\n",
    "    md = {}\n",
    "    for list in os.listdir(path):\n",
    "        \n",
    "        model.load_state_dict(torch.load(f=os.path.join(path, list), map_location=DEVICE, weights_only=True))\n",
    "        model.eval()\n",
    "        md[len(md)] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this little tutorial, to see how we handle the optimization using save states:\n",
    "\n",
    "https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/001_rdb.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-17 21:14:42,912] Using an existing study with name 'GeneratorOpti2' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using an existing study with name 'GeneratorOpti2' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name = \"GeneratorOpti2\"  # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "\n",
    "if os.path.exists(\"sampler2.pkl\"):\n",
    "    restored_sampler = pickle.load(open(\"sampler2.pkl\", \"rb\"))\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, sampler=restored_sampler)\n",
    "else:\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-18 10:07:25,595] Trial 31 finished with value: 2.5626204639673227 and parameters: {'opt_Epochs': 4, 'opt_Steps': 17, 'opt_Learning_Rate': 0.13113464986409595, 'opt_Batch_Size': 330, 'opt_Number_of_Noise_Batches': 2, 'opt_Regularization_term': 0.27166762667600725, 'opt_Noise_Dim': 58, 'n_layers': 5}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31 finished with value: 2.5626204639673227 and parameters: {'opt_Epochs': 4, 'opt_Steps': 17, 'opt_Learning_Rate': 0.13113464986409595, 'opt_Batch_Size': 330, 'opt_Number_of_Noise_Batches': 2, 'opt_Regularization_term': 0.27166762667600725, 'opt_Noise_Dim': 58, 'n_layers': 5}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-18 10:14:12,074] Trial 32 finished with value: 2.5201350718736646 and parameters: {'opt_Epochs': 1, 'opt_Steps': 11, 'opt_Learning_Rate': 0.014085645404016113, 'opt_Batch_Size': 266, 'opt_Number_of_Noise_Batches': 8, 'opt_Regularization_term': 0.1832003102316107, 'opt_Noise_Dim': 331, 'n_layers': 3}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 finished with value: 2.5201350718736646 and parameters: {'opt_Epochs': 1, 'opt_Steps': 11, 'opt_Learning_Rate': 0.014085645404016113, 'opt_Batch_Size': 266, 'opt_Number_of_Noise_Batches': 8, 'opt_Regularization_term': 0.1832003102316107, 'opt_Noise_Dim': 331, 'n_layers': 3}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-18 10:17:50,254] Trial 33 finished with value: 2.996022218465806 and parameters: {'opt_Epochs': 3, 'opt_Steps': 1, 'opt_Learning_Rate': 0.17235797723470592, 'opt_Batch_Size': 90, 'opt_Number_of_Noise_Batches': 7, 'opt_Regularization_term': 0.13947061703175456, 'opt_Noise_Dim': 240, 'n_layers': 2}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 finished with value: 2.996022218465806 and parameters: {'opt_Epochs': 3, 'opt_Steps': 1, 'opt_Learning_Rate': 0.17235797723470592, 'opt_Batch_Size': 90, 'opt_Number_of_Noise_Batches': 7, 'opt_Regularization_term': 0.13947061703175456, 'opt_Noise_Dim': 240, 'n_layers': 2}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-18 10:53:13,385] Trial 34 finished with value: 2.5245006024837497 and parameters: {'opt_Epochs': 9, 'opt_Steps': 12, 'opt_Learning_Rate': 0.11262137988854005, 'opt_Batch_Size': 399, 'opt_Number_of_Noise_Batches': 4, 'opt_Regularization_term': 0.10888334390174914, 'opt_Noise_Dim': 281, 'n_layers': 3}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34 finished with value: 2.5245006024837497 and parameters: {'opt_Epochs': 9, 'opt_Steps': 12, 'opt_Learning_Rate': 0.11262137988854005, 'opt_Batch_Size': 399, 'opt_Number_of_Noise_Batches': 4, 'opt_Regularization_term': 0.10888334390174914, 'opt_Noise_Dim': 281, 'n_layers': 3}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-18 11:05:49,320] Trial 35 finished with value: 2.5003083825111387 and parameters: {'opt_Epochs': 2, 'opt_Steps': 11, 'opt_Learning_Rate': 0.04231281786048584, 'opt_Batch_Size': 511, 'opt_Number_of_Noise_Batches': 7, 'opt_Regularization_term': 0.2428903795536915, 'opt_Noise_Dim': 143, 'n_layers': 3}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35 finished with value: 2.5003083825111387 and parameters: {'opt_Epochs': 2, 'opt_Steps': 11, 'opt_Learning_Rate': 0.04231281786048584, 'opt_Batch_Size': 511, 'opt_Number_of_Noise_Batches': 7, 'opt_Regularization_term': 0.2428903795536915, 'opt_Noise_Dim': 143, 'n_layers': 3}. Best is trial 17 with value: 2.2544206410646437.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Steps = trial.suggest_int('opt_Steps', 1, 20)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 512)\n",
    "    opt_Number_of_Noise_Batches = trial.suggest_int('opt_Number_of_Noise_Batches', 1, 10)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "\n",
    "    # print(f\"Epochs: {opt_Epochs} |\\nSteps: {opt_Steps} |\\nLearning Rate: {opt_Learning_Rate} |\\nBatch Size: {opt_Batch_Size} |\\nNoise Batches: {opt_Number_of_Noise_Batches} |\\nRegularization Term: {opt_Regularization_term} |\\nNoise Dim: {opt_Noise_Dim}\")\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 8)\n",
    "\n",
    "    Layers = [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,]\n",
    "    Layers = Layers[:n_layers]\n",
    "    # print(\"Layers: \", Layers)\n",
    "\n",
    "    mod = main(\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Steps= opt_Steps,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_Number_of_Noise_Batches = opt_Number_of_Noise_Batches,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        new_baseline=False,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "    \n",
    "    data_dir = f'data{os.sep}cifar10'\n",
    "\n",
    "    transform_test = tt.Compose([\n",
    "        tt.ToTensor(),\n",
    "        tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    valid_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "    valid_dl = DataLoader(valid_ds, 256,)\n",
    "\n",
    "    exact = resnet18(num_classes = 10)\n",
    "    n = random.randint(0, len(os.listdir(\"data/retrain/models\"))-1)\n",
    "    exact.load_state_dict(torch.load(f\"data/retrain/models/ResNET18_CIFAR10_RETRAIN_CLASSES_{n}.pt\", map_location=DEVICE, weights_only=True))\n",
    "    div = kl_divergence_between_models(\n",
    "        model1 = exact,\n",
    "        model2 = mod,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler2.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study.sampler, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opt_Epochs': 10,\n",
       " 'opt_Steps': 3,\n",
       " 'opt_Learning_Rate': 0.20640839953786477,\n",
       " 'opt_Batch_Size': 186,\n",
       " 'opt_Number_of_Noise_Batches': 5,\n",
       " 'opt_Regularization_term': 0.18035246104166439,\n",
       " 'opt_Noise_Dim': 426,\n",
       " 'n_layers': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_n_layers</th>\n",
       "      <th>params_opt_Batch_Size</th>\n",
       "      <th>params_opt_Epochs</th>\n",
       "      <th>params_opt_Learning_Rate</th>\n",
       "      <th>params_opt_Noise_Dim</th>\n",
       "      <th>params_opt_Number_of_Noise_Batches</th>\n",
       "      <th>params_opt_Regularization_term</th>\n",
       "      <th>params_opt_Steps</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.305901</td>\n",
       "      <td>2025-01-17 12:31:05.669089</td>\n",
       "      <td>2025-01-17 12:31:37.217590</td>\n",
       "      <td>0 days 00:00:31.548501</td>\n",
       "      <td>3</td>\n",
       "      <td>186</td>\n",
       "      <td>10</td>\n",
       "      <td>0.206408</td>\n",
       "      <td>426</td>\n",
       "      <td>5</td>\n",
       "      <td>0.180352</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.702530</td>\n",
       "      <td>2025-01-17 12:10:48.627085</td>\n",
       "      <td>2025-01-17 12:12:47.792270</td>\n",
       "      <td>0 days 00:01:59.165185</td>\n",
       "      <td>6</td>\n",
       "      <td>423</td>\n",
       "      <td>3</td>\n",
       "      <td>0.093255</td>\n",
       "      <td>382</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>13</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.749585</td>\n",
       "      <td>2025-01-17 12:09:47.814136</td>\n",
       "      <td>2025-01-17 12:10:48.611890</td>\n",
       "      <td>0 days 00:01:00.797754</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.167193</td>\n",
       "      <td>225</td>\n",
       "      <td>7</td>\n",
       "      <td>0.108306</td>\n",
       "      <td>12</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2.812649</td>\n",
       "      <td>2025-01-17 21:14:43.036284</td>\n",
       "      <td>2025-01-17 21:42:59.640412</td>\n",
       "      <td>0 days 00:28:16.604128</td>\n",
       "      <td>4</td>\n",
       "      <td>179</td>\n",
       "      <td>6</td>\n",
       "      <td>0.158649</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031941</td>\n",
       "      <td>20</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.958736</td>\n",
       "      <td>2025-01-17 12:15:02.580302</td>\n",
       "      <td>2025-01-17 12:15:42.109564</td>\n",
       "      <td>0 days 00:00:39.529262</td>\n",
       "      <td>2</td>\n",
       "      <td>450</td>\n",
       "      <td>3</td>\n",
       "      <td>0.048239</td>\n",
       "      <td>197</td>\n",
       "      <td>4</td>\n",
       "      <td>0.086577</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.031566</td>\n",
       "      <td>2025-01-17 12:15:42.126696</td>\n",
       "      <td>2025-01-17 12:21:24.702130</td>\n",
       "      <td>0 days 00:05:42.575434</td>\n",
       "      <td>8</td>\n",
       "      <td>471</td>\n",
       "      <td>6</td>\n",
       "      <td>0.297750</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.207947</td>\n",
       "      <td>19</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.169060</td>\n",
       "      <td>2025-01-17 12:31:37.233993</td>\n",
       "      <td>2025-01-17 12:33:19.228269</td>\n",
       "      <td>0 days 00:01:41.994276</td>\n",
       "      <td>4</td>\n",
       "      <td>458</td>\n",
       "      <td>9</td>\n",
       "      <td>0.096268</td>\n",
       "      <td>448</td>\n",
       "      <td>9</td>\n",
       "      <td>0.076254</td>\n",
       "      <td>10</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-17 12:08:55.005249</td>\n",
       "      <td>2025-01-17 12:09:16.458867</td>\n",
       "      <td>0 days 00:00:21.453618</td>\n",
       "      <td>7</td>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>0.025044</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166994</td>\n",
       "      <td>3</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-17 20:57:22.915080</td>\n",
       "      <td>2025-01-17 20:57:24.298970</td>\n",
       "      <td>0 days 00:00:01.383890</td>\n",
       "      <td>4</td>\n",
       "      <td>179</td>\n",
       "      <td>6</td>\n",
       "      <td>0.158649</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031941</td>\n",
       "      <td>20</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-17 21:11:11.779227</td>\n",
       "      <td>2025-01-17 21:11:13.149912</td>\n",
       "      <td>0 days 00:00:01.370685</td>\n",
       "      <td>4</td>\n",
       "      <td>179</td>\n",
       "      <td>6</td>\n",
       "      <td>0.158649</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031941</td>\n",
       "      <td>20</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "5        5  2.305901 2025-01-17 12:31:05.669089 2025-01-17 12:31:37.217590   \n",
       "2        2  2.702530 2025-01-17 12:10:48.627085 2025-01-17 12:12:47.792270   \n",
       "1        1  2.749585 2025-01-17 12:09:47.814136 2025-01-17 12:10:48.611890   \n",
       "10      10  2.812649 2025-01-17 21:14:43.036284 2025-01-17 21:42:59.640412   \n",
       "3        3  2.958736 2025-01-17 12:15:02.580302 2025-01-17 12:15:42.109564   \n",
       "4        4  3.031566 2025-01-17 12:15:42.126696 2025-01-17 12:21:24.702130   \n",
       "6        6  3.169060 2025-01-17 12:31:37.233993 2025-01-17 12:33:19.228269   \n",
       "0        0       NaN 2025-01-17 12:08:55.005249 2025-01-17 12:09:16.458867   \n",
       "7        7       NaN 2025-01-17 20:57:22.915080 2025-01-17 20:57:24.298970   \n",
       "8        8       NaN 2025-01-17 21:11:11.779227 2025-01-17 21:11:13.149912   \n",
       "\n",
       "                 duration  params_n_layers  params_opt_Batch_Size  \\\n",
       "5  0 days 00:00:31.548501                3                    186   \n",
       "2  0 days 00:01:59.165185                6                    423   \n",
       "1  0 days 00:01:00.797754                3                     64   \n",
       "10 0 days 00:28:16.604128                4                    179   \n",
       "3  0 days 00:00:39.529262                2                    450   \n",
       "4  0 days 00:05:42.575434                8                    471   \n",
       "6  0 days 00:01:41.994276                4                    458   \n",
       "0  0 days 00:00:21.453618                7                    116   \n",
       "7  0 days 00:00:01.383890                4                    179   \n",
       "8  0 days 00:00:01.370685                4                    179   \n",
       "\n",
       "    params_opt_Epochs  params_opt_Learning_Rate  params_opt_Noise_Dim  \\\n",
       "5                  10                  0.206408                   426   \n",
       "2                   3                  0.093255                   382   \n",
       "1                   9                  0.167193                   225   \n",
       "10                  6                  0.158649                    52   \n",
       "3                   3                  0.048239                   197   \n",
       "4                   6                  0.297750                     8   \n",
       "6                   9                  0.096268                   448   \n",
       "0                   2                  0.025044                   249   \n",
       "7                   6                  0.158649                    52   \n",
       "8                   6                  0.158649                    52   \n",
       "\n",
       "    params_opt_Number_of_Noise_Batches  params_opt_Regularization_term  \\\n",
       "5                                    5                        0.180352   \n",
       "2                                    4                        0.043100   \n",
       "1                                    7                        0.108306   \n",
       "10                                   6                        0.031941   \n",
       "3                                    4                        0.086577   \n",
       "4                                    7                        0.207947   \n",
       "6                                    9                        0.076254   \n",
       "0                                    1                        0.166994   \n",
       "7                                    6                        0.031941   \n",
       "8                                    6                        0.031941   \n",
       "\n",
       "    params_opt_Steps     state  \n",
       "5                  3  COMPLETE  \n",
       "2                 13  COMPLETE  \n",
       "1                 12  COMPLETE  \n",
       "10                20  COMPLETE  \n",
       "3                  1  COMPLETE  \n",
       "4                 19  COMPLETE  \n",
       "6                 10  COMPLETE  \n",
       "0                  3      FAIL  \n",
       "7                 20      FAIL  \n",
       "8                 20      FAIL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials_df = study.trials_dataframe()\n",
    "best10_df = trials_df.sort_values(\"value\").head(10)\n",
    "best10_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 2.818575150200299\n",
      "Number of Layers 4.5 \n",
      "Batch Size for Training 270.5 \n",
      "Epochs for Noise Training 6.0 \n",
      "LR for Noise Training 0.14101033415351125 \n",
      "Noise Dim of Generator 209.1\n",
      "Number of Noise Batches Used 5.5 \n",
      "Regularization Term 0.09653515835859165 \n",
      "Learning Steps for Noise Training 12.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value {float(best10_df[\"value\"].mean())}\")\n",
    "print(f\"Number of Layers {float(best10_df[\"params_n_layers\"].mean())} \")\n",
    "print(f\"Batch Size for Training {float(best10_df[\"params_opt_Batch_Size\"].mean())} \")\n",
    "print(f\"Epochs for Noise Training {float(best10_df[\"params_opt_Epochs\"].mean())} \")\n",
    "print(f\"LR for Noise Training {float(best10_df[\"params_opt_Learning_Rate\"].mean())} \")\n",
    "print(f\"Noise Dim of Generator {float(best10_df[\"params_opt_Noise_Dim\"].mean())}\")\n",
    "print(f\"Number of Noise Batches Used {float(best10_df[\"params_opt_Number_of_Noise_Batches\"].mean())} \")\n",
    "print(f\"Regularization Term {float(best10_df[\"params_opt_Regularization_term\"].mean())} \")\n",
    "print(f\"Learning Steps for Noise Training {float(best10_df[\"params_opt_Steps\"].mean())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^This will represent the values used as default^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Standard Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n0 = 5000\n",
    "# n2 = 5000\n",
    "# batch_size = 128\n",
    "\n",
    "# standard_model, standard_history = main(\n",
    "#     t_Epochs = 5,\n",
    "#     t_Steps= int((n0 + n2)/(2 * batch_size)), # The Idea is to have the same amount of updates as their are samples to unlearn\n",
    "#     t_Learning_Rate = 0.1,\n",
    "#     t_Batch_Size = batch_size,\n",
    "#     t_Number_of_Noise_Batches = 10,\n",
    "#     t_Regularization_term = 0.1,\n",
    "#     t_Layers = [1000],\n",
    "#     t_Noise_Dim = 100,\n",
    "#     new_baseline=True,\n",
    "#     logs=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_ms = load_models_dict(path=\"data/all/models\")\n",
    "\n",
    "exact_ms = load_models_dict(path=\"data/retrain/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import kl_divergence_between_models\n",
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "batch_size = 256\n",
    "data_dir = f'data{os.sep}cifar10'\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "valid_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=False)\n",
    "\n",
    "kl_divergence_between_models(model1 = train_ms[0], model2 = train_ms[0], data_loader = valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=exact_ms[0], model2=exact_ms[0], data_loader=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=train_ms[0], model2=exact_ms[0], data_loader=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=exact_ms[0], model2=train_ms[0], data_loader=valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
