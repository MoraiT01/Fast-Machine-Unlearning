{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Finetuning\n",
    "\n",
    "We want to find the right parameters for the Generator Network.\n",
    "\n",
    "By using the best values for the following parameters:\n",
    "\n",
    "- Number of Epochs (meaning `num_epochs` and `num_steps`)\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Number of Noise Batches\n",
    "- Number of Layers\n",
    "- Regularization term\n",
    "- Number of Neurons for each Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fyemu_tunable import main, evaluate\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import optuna\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from src.metrics import kl_divergence_between_models\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this little tutorial, to see how we handle the optimization using save states:\n",
    "\n",
    "https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/001_rdb.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name = \"GeneratorOpti\"  # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "\n",
    "if os.path.exists(\"sampler.pkl\"):\n",
    "    restored_sampler = pickle.load(open(\"sampler.pkl\", \"rb\"))\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, sampler=restored_sampler)\n",
    "else:\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Steps = trial.suggest_int('opt_Steps', 1, 20)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = 256 # trial.suggest_int('opt_Batch_Size', 32, 512)\n",
    "    opt_Number_of_Noise_Batches = trial.suggest_int('opt_Number_of_Noise_Batches', 1, 10)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "\n",
    "    # print(f\"Epochs: {opt_Epochs} |\\nSteps: {opt_Steps} |\\nLearning Rate: {opt_Learning_Rate} |\\nBatch Size: {opt_Batch_Size} |\\nNoise Batches: {opt_Number_of_Noise_Batches} |\\nRegularization Term: {opt_Regularization_term} |\\nNoise Dim: {opt_Noise_Dim}\")\n",
    "\n",
    "    l1 = trial.suggest_int('l1', 32, 1024)\n",
    "    l2 = trial.suggest_int('l2', 32, 1024)\n",
    "    l3 = trial.suggest_int('l3', 32, 1024)\n",
    "    l4 = trial.suggest_int('l4', 32, 1024)\n",
    "    l5 = trial.suggest_int('l5', 32, 1024)\n",
    "    l6 = trial.suggest_int('l6', 32, 1024)\n",
    "    l7 = trial.suggest_int('l7', 32, 1024)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 7)\n",
    "\n",
    "    Layers = [l1, l2, l3, l4, l5, l6, l7]\n",
    "    Layers = Layers[:n_layers]\n",
    "    # print(\"Layers: \", Layers)\n",
    "\n",
    "    mod = main(\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Steps= opt_Steps,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_Number_of_Noise_Batches = opt_Number_of_Noise_Batches,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        new_baseline=False,\n",
    "        logs=False,\n",
    "        model_eval_logs=True,\n",
    "    )\n",
    "    \n",
    "    data_dir = f'data{os.sep}cifar10'\n",
    "\n",
    "    transform_test = tt.Compose([\n",
    "        tt.ToTensor(),\n",
    "        tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    valid_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "    valid_dl = DataLoader(valid_ds, 256,)\n",
    "\n",
    "    exact = resnet18(num_classes = 10)\n",
    "    exact.load_state_dict(torch.load(\"ResNET18_CIFAR10_RETAIN_CLASSES.pt\", weights_only=True))\n",
    "    div = kl_divergence_between_models(\n",
    "        model1 = mod,\n",
    "        model2 = exact,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study.sampler, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opt_Epochs': 7,\n",
       " 'opt_Steps': 5,\n",
       " 'opt_Learning_Rate': 0.1554530512278481,\n",
       " 'opt_Batch_Size': 273,\n",
       " 'opt_Number_of_Noise_Batches': 3,\n",
       " 'opt_Regularization_term': 0.20414346598354305,\n",
       " 'opt_Noise_Dim': 229,\n",
       " 'l1': 672,\n",
       " 'l2': 408,\n",
       " 'l3': 266,\n",
       " 'l4': 129,\n",
       " 'l5': 372,\n",
       " 'l6': 443,\n",
       " 'l7': 528,\n",
       " 'n_layers': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_l1</th>\n",
       "      <th>params_l2</th>\n",
       "      <th>params_l3</th>\n",
       "      <th>params_l4</th>\n",
       "      <th>params_l5</th>\n",
       "      <th>...</th>\n",
       "      <th>params_l7</th>\n",
       "      <th>params_n_layers</th>\n",
       "      <th>params_opt_Batch_Size</th>\n",
       "      <th>params_opt_Epochs</th>\n",
       "      <th>params_opt_Learning_Rate</th>\n",
       "      <th>params_opt_Noise_Dim</th>\n",
       "      <th>params_opt_Number_of_Noise_Batches</th>\n",
       "      <th>params_opt_Regularization_term</th>\n",
       "      <th>params_opt_Steps</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>275</td>\n",
       "      <td>0.768493</td>\n",
       "      <td>2025-01-14 10:31:35.793537</td>\n",
       "      <td>2025-01-14 10:33:28.905537</td>\n",
       "      <td>0 days 00:01:53.112000</td>\n",
       "      <td>672</td>\n",
       "      <td>408</td>\n",
       "      <td>266</td>\n",
       "      <td>129</td>\n",
       "      <td>372</td>\n",
       "      <td>...</td>\n",
       "      <td>528</td>\n",
       "      <td>3</td>\n",
       "      <td>273.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.155453</td>\n",
       "      <td>229</td>\n",
       "      <td>3</td>\n",
       "      <td>0.204143</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>0.778053</td>\n",
       "      <td>2025-01-13 09:19:32.307524</td>\n",
       "      <td>2025-01-13 09:21:30.359889</td>\n",
       "      <td>0 days 00:01:58.052365</td>\n",
       "      <td>534</td>\n",
       "      <td>910</td>\n",
       "      <td>538</td>\n",
       "      <td>155</td>\n",
       "      <td>958</td>\n",
       "      <td>...</td>\n",
       "      <td>535</td>\n",
       "      <td>5</td>\n",
       "      <td>201.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.036812</td>\n",
       "      <td>479</td>\n",
       "      <td>8</td>\n",
       "      <td>0.147283</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>570</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>2025-01-16 10:05:56.810646</td>\n",
       "      <td>2025-01-16 10:08:11.953031</td>\n",
       "      <td>0 days 00:02:15.142385</td>\n",
       "      <td>368</td>\n",
       "      <td>336</td>\n",
       "      <td>439</td>\n",
       "      <td>781</td>\n",
       "      <td>429</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>0.165617</td>\n",
       "      <td>336</td>\n",
       "      <td>8</td>\n",
       "      <td>0.061542</td>\n",
       "      <td>19</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>261</td>\n",
       "      <td>0.778298</td>\n",
       "      <td>2025-01-14 10:03:38.835062</td>\n",
       "      <td>2025-01-14 10:05:32.148857</td>\n",
       "      <td>0 days 00:01:53.313795</td>\n",
       "      <td>692</td>\n",
       "      <td>314</td>\n",
       "      <td>238</td>\n",
       "      <td>210</td>\n",
       "      <td>598</td>\n",
       "      <td>...</td>\n",
       "      <td>517</td>\n",
       "      <td>3</td>\n",
       "      <td>298.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.158513</td>\n",
       "      <td>76</td>\n",
       "      <td>8</td>\n",
       "      <td>0.212575</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>371</td>\n",
       "      <td>0.778931</td>\n",
       "      <td>2025-01-14 13:31:40.866109</td>\n",
       "      <td>2025-01-14 13:33:37.786496</td>\n",
       "      <td>0 days 00:01:56.920387</td>\n",
       "      <td>349</td>\n",
       "      <td>63</td>\n",
       "      <td>327</td>\n",
       "      <td>157</td>\n",
       "      <td>360</td>\n",
       "      <td>...</td>\n",
       "      <td>228</td>\n",
       "      <td>7</td>\n",
       "      <td>269.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.036099</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "      <td>0.204727</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>204</td>\n",
       "      <td>0.779018</td>\n",
       "      <td>2025-01-13 11:32:50.460912</td>\n",
       "      <td>2025-01-13 11:34:42.504825</td>\n",
       "      <td>0 days 00:01:52.043913</td>\n",
       "      <td>475</td>\n",
       "      <td>270</td>\n",
       "      <td>208</td>\n",
       "      <td>137</td>\n",
       "      <td>625</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>3</td>\n",
       "      <td>227.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.013843</td>\n",
       "      <td>331</td>\n",
       "      <td>10</td>\n",
       "      <td>0.246037</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>0.779788</td>\n",
       "      <td>2025-01-09 11:17:04.540813</td>\n",
       "      <td>2025-01-09 11:17:34.150099</td>\n",
       "      <td>0 days 00:00:29.609286</td>\n",
       "      <td>312</td>\n",
       "      <td>358</td>\n",
       "      <td>718</td>\n",
       "      <td>149</td>\n",
       "      <td>886</td>\n",
       "      <td>...</td>\n",
       "      <td>595</td>\n",
       "      <td>5</td>\n",
       "      <td>189.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.027585</td>\n",
       "      <td>133</td>\n",
       "      <td>7</td>\n",
       "      <td>0.257624</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>0.780311</td>\n",
       "      <td>2025-01-14 14:16:19.846509</td>\n",
       "      <td>2025-01-14 14:18:16.838864</td>\n",
       "      <td>0 days 00:01:56.992355</td>\n",
       "      <td>677</td>\n",
       "      <td>59</td>\n",
       "      <td>243</td>\n",
       "      <td>106</td>\n",
       "      <td>613</td>\n",
       "      <td>...</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>272.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.042272</td>\n",
       "      <td>263</td>\n",
       "      <td>6</td>\n",
       "      <td>0.189973</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>0.783781</td>\n",
       "      <td>2025-01-15 16:13:45.013982</td>\n",
       "      <td>2025-01-15 16:16:27.170467</td>\n",
       "      <td>0 days 00:02:42.156485</td>\n",
       "      <td>700</td>\n",
       "      <td>797</td>\n",
       "      <td>312</td>\n",
       "      <td>172</td>\n",
       "      <td>594</td>\n",
       "      <td>...</td>\n",
       "      <td>496</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>0.142858</td>\n",
       "      <td>502</td>\n",
       "      <td>10</td>\n",
       "      <td>0.215919</td>\n",
       "      <td>19</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>526</td>\n",
       "      <td>0.784351</td>\n",
       "      <td>2025-01-15 17:14:41.868992</td>\n",
       "      <td>2025-01-15 17:17:16.566668</td>\n",
       "      <td>0 days 00:02:34.697676</td>\n",
       "      <td>485</td>\n",
       "      <td>344</td>\n",
       "      <td>312</td>\n",
       "      <td>143</td>\n",
       "      <td>685</td>\n",
       "      <td>...</td>\n",
       "      <td>789</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>0.039318</td>\n",
       "      <td>399</td>\n",
       "      <td>6</td>\n",
       "      <td>0.082118</td>\n",
       "      <td>16</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     number     value             datetime_start          datetime_complete  \\\n",
       "275     275  0.768493 2025-01-14 10:31:35.793537 2025-01-14 10:33:28.905537   \n",
       "135     135  0.778053 2025-01-13 09:19:32.307524 2025-01-13 09:21:30.359889   \n",
       "570     570  0.778113 2025-01-16 10:05:56.810646 2025-01-16 10:08:11.953031   \n",
       "261     261  0.778298 2025-01-14 10:03:38.835062 2025-01-14 10:05:32.148857   \n",
       "371     371  0.778931 2025-01-14 13:31:40.866109 2025-01-14 13:33:37.786496   \n",
       "204     204  0.779018 2025-01-13 11:32:50.460912 2025-01-13 11:34:42.504825   \n",
       "91       91  0.779788 2025-01-09 11:17:04.540813 2025-01-09 11:17:34.150099   \n",
       "395     395  0.780311 2025-01-14 14:16:19.846509 2025-01-14 14:18:16.838864   \n",
       "500     500  0.783781 2025-01-15 16:13:45.013982 2025-01-15 16:16:27.170467   \n",
       "526     526  0.784351 2025-01-15 17:14:41.868992 2025-01-15 17:17:16.566668   \n",
       "\n",
       "                  duration  params_l1  params_l2  params_l3  params_l4  \\\n",
       "275 0 days 00:01:53.112000        672        408        266        129   \n",
       "135 0 days 00:01:58.052365        534        910        538        155   \n",
       "570 0 days 00:02:15.142385        368        336        439        781   \n",
       "261 0 days 00:01:53.313795        692        314        238        210   \n",
       "371 0 days 00:01:56.920387        349         63        327        157   \n",
       "204 0 days 00:01:52.043913        475        270        208        137   \n",
       "91  0 days 00:00:29.609286        312        358        718        149   \n",
       "395 0 days 00:01:56.992355        677         59        243        106   \n",
       "500 0 days 00:02:42.156485        700        797        312        172   \n",
       "526 0 days 00:02:34.697676        485        344        312        143   \n",
       "\n",
       "     params_l5  ...  params_l7  params_n_layers  params_opt_Batch_Size  \\\n",
       "275        372  ...        528                3                  273.0   \n",
       "135        958  ...        535                5                  201.0   \n",
       "570        429  ...         49                1                    NaN   \n",
       "261        598  ...        517                3                  298.0   \n",
       "371        360  ...        228                7                  269.0   \n",
       "204        625  ...        204                3                  227.0   \n",
       "91         886  ...        595                5                  189.0   \n",
       "395        613  ...        161                3                  272.0   \n",
       "500        594  ...        496                4                    NaN   \n",
       "526        685  ...        789                4                    NaN   \n",
       "\n",
       "     params_opt_Epochs  params_opt_Learning_Rate  params_opt_Noise_Dim  \\\n",
       "275                  7                  0.155453                   229   \n",
       "135                  8                  0.036812                   479   \n",
       "570                  9                  0.165617                   336   \n",
       "261                  7                  0.158513                    76   \n",
       "371                  9                  0.036099                   427   \n",
       "204                  8                  0.013843                   331   \n",
       "91                   6                  0.027585                   133   \n",
       "395                  9                  0.042272                   263   \n",
       "500                  8                  0.142858                   502   \n",
       "526                  8                  0.039318                   399   \n",
       "\n",
       "     params_opt_Number_of_Noise_Batches  params_opt_Regularization_term  \\\n",
       "275                                   3                        0.204143   \n",
       "135                                   8                        0.147283   \n",
       "570                                   8                        0.061542   \n",
       "261                                   8                        0.212575   \n",
       "371                                   2                        0.204727   \n",
       "204                                  10                        0.246037   \n",
       "91                                    7                        0.257624   \n",
       "395                                   6                        0.189973   \n",
       "500                                  10                        0.215919   \n",
       "526                                   6                        0.082118   \n",
       "\n",
       "     params_opt_Steps     state  \n",
       "275                 5  COMPLETE  \n",
       "135                 5  COMPLETE  \n",
       "570                19  COMPLETE  \n",
       "261                 3  COMPLETE  \n",
       "371                 3  COMPLETE  \n",
       "204                 3  COMPLETE  \n",
       "91                  3  COMPLETE  \n",
       "395                 4  COMPLETE  \n",
       "500                19  COMPLETE  \n",
       "526                16  COMPLETE  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials_df = study.trials_dataframe()\n",
    "best10_df = trials_df.sort_values(\"value\").head(10)\n",
    "best10_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 0.7789136573299765\n",
      "L1 526.4 \n",
      "L2 385.9 \n",
      "L3 360.1 \n",
      "L4 213.9 \n",
      "L5 612.0 \n",
      "L6 377.2 \n",
      "L7 410.2 \n",
      "Number of Layers 3.8 \n",
      "Batch Size for Training 247.0 \n",
      "Epochs for Noise Training 7.9 \n",
      "LR for Noise Training 0.0818370551051665 \n",
      "Noise Dim of Generator 317.5\n",
      "Number of Noise Batches Used 6.8 \n",
      "Regularization Term 0.18219432382942993 \n",
      "Learning Steps for Noise Training 8.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value {float(best10_df[\"value\"].mean())}\")\n",
    "print(f\"L1 {float(best10_df[\"params_l1\"].mean())} \")\n",
    "print(f\"L2 {float(best10_df[\"params_l2\"].mean())} \")\n",
    "print(f\"L3 {float(best10_df[\"params_l3\"].mean())} \")\n",
    "print(f\"L4 {float(best10_df[\"params_l4\"].mean())} \")\n",
    "print(f\"L5 {float(best10_df[\"params_l5\"].mean())} \")\n",
    "print(f\"L6 {float(best10_df[\"params_l6\"].mean())} \")\n",
    "print(f\"L7 {float(best10_df[\"params_l7\"].mean())} \")\n",
    "print(f\"Number of Layers {float(best10_df[\"params_n_layers\"].mean())} \")\n",
    "print(f\"Batch Size for Training {float(best10_df[\"params_opt_Batch_Size\"].mean())} \")\n",
    "print(f\"Epochs for Noise Training {float(best10_df[\"params_opt_Epochs\"].mean())} \")\n",
    "print(f\"LR for Noise Training {float(best10_df[\"params_opt_Learning_Rate\"].mean())} \")\n",
    "print(f\"Noise Dim of Generator {float(best10_df[\"params_opt_Noise_Dim\"].mean())}\")\n",
    "print(f\"Number of Noise Batches Used {float(best10_df[\"params_opt_Number_of_Noise_Batches\"].mean())} \")\n",
    "print(f\"Regularization Term {float(best10_df[\"params_opt_Regularization_term\"].mean())} \")\n",
    "print(f\"Learning Steps for Noise Training {float(best10_df[\"params_opt_Steps\"].mean())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^This will represent the values used as default^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Standard Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n0 = 5000\n",
    "# n2 = 5000\n",
    "# batch_size = 128\n",
    "\n",
    "# standard_model, standard_history = main(\n",
    "#     t_Epochs = 5,\n",
    "#     t_Steps= int((n0 + n2)/(2 * batch_size)), # The Idea is to have the same amount of updates as their are samples to unlearn\n",
    "#     t_Learning_Rate = 0.1,\n",
    "#     t_Batch_Size = batch_size,\n",
    "#     t_Number_of_Noise_Batches = 10,\n",
    "#     t_Regularization_term = 0.1,\n",
    "#     t_Layers = [1000],\n",
    "#     t_Noise_Dim = 100,\n",
    "#     new_baseline=True,\n",
    "#     logs=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train = resnet18(num_classes = 10).to(DEVICE)\n",
    "train.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\",     weights_only=True))\n",
    "\n",
    "exact = resnet18(num_classes = 10).to(DEVICE)\n",
    "exact.load_state_dict(torch.load(\"ResNET18_CIFAR10_RETAIN_CLASSES.pt\",  weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import kl_divergence_between_models\n",
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "batch_size = 256\n",
    "data_dir = f'data{os.sep}cifar10'\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "valid_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=False)\n",
    "\n",
    "kl_divergence_between_models(model1 = train, model2 = train, data_loader = valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=exact, model2=exact, data_loader=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=train, model2=exact, data_loader=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=exact, model2=train, data_loader=valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
