{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Finetuning\n",
    "\n",
    "We want to find the right parameters for the Generator Network.\n",
    "\n",
    "By using the best values for the following parameters:\n",
    "\n",
    "- Number of Epochs (meaning `num_epochs` and `num_steps`)\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Number of Noise Batches\n",
    "- Number of Layers\n",
    "- Regularization term\n",
    "- Number of Neurons for each Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fyemu_tunable import main, evaluate\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import optuna\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from src.metrics import kl_divergence_between_models\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this little tutorial, to see how we handle the optimization using save states:\n",
    "\n",
    "https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/001_rdb.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name = \"GeneratorOpti\"  # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "\n",
    "if os.path.exists(\"sampler.pkl\"):\n",
    "    restored_sampler = pickle.load(open(\"sampler.pkl\", \"rb\"))\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, sampler=restored_sampler)\n",
    "else:\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Steps = trial.suggest_int('opt_Steps', 1, 20)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 512)\n",
    "    opt_Number_of_Noise_Batches = trial.suggest_int('opt_Number_of_Noise_Batches', 1, 10)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "\n",
    "    # print(f\"Epochs: {opt_Epochs} |\\nSteps: {opt_Steps} |\\nLearning Rate: {opt_Learning_Rate} |\\nBatch Size: {opt_Batch_Size} |\\nNoise Batches: {opt_Number_of_Noise_Batches} |\\nRegularization Term: {opt_Regularization_term} |\\nNoise Dim: {opt_Noise_Dim}\")\n",
    "\n",
    "    l1 = trial.suggest_int('l1', 32, 1024)\n",
    "    l2 = trial.suggest_int('l2', 32, 1024)\n",
    "    l3 = trial.suggest_int('l3', 32, 1024)\n",
    "    l4 = trial.suggest_int('l4', 32, 1024)\n",
    "    l5 = trial.suggest_int('l5', 32, 1024)\n",
    "    l6 = trial.suggest_int('l6', 32, 1024)\n",
    "    l7 = trial.suggest_int('l7', 32, 1024)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 7)\n",
    "\n",
    "    Layers = [l1, l2, l3, l4, l5, l6, l7]\n",
    "    Layers = Layers[:n_layers]\n",
    "    # print(\"Layers: \", Layers)\n",
    "\n",
    "    mod = main(\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Steps= opt_Steps,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_Number_of_Noise_Batches = opt_Number_of_Noise_Batches,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        new_baseline=False,\n",
    "        logs=False,\n",
    "        model_eval_logs=True,\n",
    "    )\n",
    "    \n",
    "    data_dir = f'data{os.sep}cifar10'\n",
    "\n",
    "    transform_test = tt.Compose([\n",
    "        tt.ToTensor(),\n",
    "        tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    valid_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "    valid_dl = DataLoader(valid_ds, 256,)\n",
    "\n",
    "    exact = resnet18(num_classes = 10)\n",
    "    exact.load_state_dict(torch.load(\"ResNET18_CIFAR10_RETAIN_CLASSES.pt\", weights_only=True))\n",
    "    div = kl_divergence_between_models(\n",
    "        model1 = mod,\n",
    "        model2 = exact,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study.sampler, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Standard Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n0 = 5000\n",
    "# n2 = 5000\n",
    "# batch_size = 128\n",
    "\n",
    "# standard_model, standard_history = main(\n",
    "#     t_Epochs = 5,\n",
    "#     t_Steps= int((n0 + n2)/(2 * batch_size)), # The Idea is to have the same amount of updates as their are samples to unlearn\n",
    "#     t_Learning_Rate = 0.1,\n",
    "#     t_Batch_Size = batch_size,\n",
    "#     t_Number_of_Noise_Batches = 10,\n",
    "#     t_Regularization_term = 0.1,\n",
    "#     t_Layers = [1000],\n",
    "#     t_Noise_Dim = 100,\n",
    "#     new_baseline=True,\n",
    "#     logs=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train = resnet18(num_classes = 10).to(DEVICE)\n",
    "train.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\",     weights_only=True))\n",
    "\n",
    "exact = resnet18(num_classes = 10).to(DEVICE)\n",
    "exact.load_state_dict(torch.load(\"ResNET18_CIFAR10_RETAIN_CLASSES.pt\",  weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import kl_divergence_between_models\n",
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "batch_size = 256\n",
    "data_dir = f'data{os.sep}cifar10'\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "valid_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=False)\n",
    "\n",
    "kl_divergence_between_models(model1 = train, model2 = train, data_loader = valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=exact, model2=exact, data_loader=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=train, model2=exact, data_loader=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence_between_models(model1=exact, model2=train, data_loader=valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
